# Emergent Abilities of Large Language Models

- [[深入理解语言模型的突现能力]](https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f)
- [[拆解追溯 GPT-3.5 各项能力的起源]](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)
- [[Towards Complex Reasoning: the Polaris of Large Language Models]](https://yaofu.notion.site/Towards-Complex-Reasoning-the-Polaris-of-Large-Language-Models-c2b4a51355b44764975f88e6a42d4e75)


## In-Context Learning
- **[GPT3]** [Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf), NeurIPS 2020
- [Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity](https://aclanthology.org/2022.acl-long.556/), ACL2022
- [Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)
- [Transformers learn in-context by gradient descent](https://arxiv.org/abs/2212.07677)
- [Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning](https://arxiv.org/abs/2301.11916)


## Instruction tuning
- **[InstructGPT]** [Training language models to follow instructions with human feedback](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf), NeurIPS2022
- **[T0]** [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://openreview.net/pdf?id=9Vrb9D0WI4), ICLR2022
- **[Flan-T5/PaLM]** [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)
- **[Flan2020]** [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/abs/2301.13688)
- [InstructDial: Improving Zero and Few-shot Generalization in Dialogue](https://aclanthology.org/2022.emnlp-main.33/), EMNLP2022
- [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/abs/2305.02301),ACL2023finding

## Chain of Thought
- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf), NeurIPS 2022
- [Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models](https://arxiv.org/abs/2302.00618)
- [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171), ICLR2023
- [Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning](https://arxiv.org/abs/2301.11916)
- [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923)
- [Complexity-Based Prompting for Multi-Step Reasoning](https://arxiv.org/abs/2210.00720)

## Generated Data/Instruction/Knowledge
- [Generated Knowledge Prompting for Commonsense Reasoning](https://aclanthology.org/2022.acl-long.225/),ACL2022
- [Generating Training Data with Language Models: Towards Zero-Shot Language Understanding](https://proceedings.neurips.cc/paper_files/paper/2022/file/0346c148ba1c21c6b4780a961ea141dc-Paper-Conference.pdf), NeurIPS2022
- [SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions](https://arxiv.org/abs/2212.10560)
- [Generate rather than Retrieve: Large Language Models are Strong Context Generators](https://arxiv.org/abs/2209.10063), ICLR2023


