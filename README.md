# Emergent-Abilities-of-Large-Language-Models

- [[深入理解语言模型的突现能力]](https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f)
- [[拆解追溯 GPT-3.5 各项能力的起源]](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)


## In-Context Learning
- **[GPT3]** Language Models are Few-Shot Learners
- Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity
- Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers
- Transformers learn in-context by gradient descent
- Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning


## Instruction tuning
- **[InstructGPT]** Training language models to follow instructions with human feedback
- **[T0]** MULTITASK PROMPTED TRAINING ENABLES ZERO-SHOT TASK GENERALIZATION
- **[Flan-T5/PaLM]** Scaling Instruction-Finetuned Language Models
- **[Flan2020]** The Flan Collection: Designing Data and Methods for Effective Instruction Tuning
- InstructDial: Improving Zero and Few-shot Generalization in Dialogue

## Chain of Thought
- Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
- Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models
- SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT
- Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning
- Multimodal Chain-of-Thought Reasoning in Language Models
- COMPLEXITY-BASED PROMPTING FOR MULTI-STEP REASONING

## Generated Data/Instruction/Knowledge
- Generated Knowledge Prompting for Commonsense Reasoning

- Generating Training Data with Language Models: Towards Zero-Shot Language Understanding
- SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions
- GENERATE RATHER THAN RETRIEVE: LARGE LANGUAGE MODELS ARE STRONG CONTEXT GENERATORS



