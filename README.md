# Emergent Abilities of Large Language Models

- [[深入理解语言模型的突现能力]](https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f)
- [[拆解追溯 GPT-3.5 各项能力的起源]](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)


## In-Context Learning
- **[GPT3]** [Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf), NeurIPS 2020
- [Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity](https://aclanthology.org/2022.acl-long.556/), ACL2022
- [Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)
- [Transformers learn in-context by gradient descent](https://arxiv.org/abs/2212.07677)
- [Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning](https://arxiv.org/abs/2301.11916)


## Instruction tuning
- **[InstructGPT]** [Training language models to follow instructions with human feedback](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf), NeurIPS 2022
- **[T0]** [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://openreview.net/pdf?id=9Vrb9D0WI4), ICLR2022
- **[Flan-T5/PaLM]** [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)
- **[Flan2020]** [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/abs/2301.13688)
- [InstructDial: Improving Zero and Few-shot Generalization in Dialogue](https://aclanthology.org/2022.emnlp-main.33/), EMNLP2022

## Chain of Thought
- Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
- Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models
- SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT
- Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning
- Multimodal Chain-of-Thought Reasoning in Language Models
- COMPLEXITY-BASED PROMPTING FOR MULTI-STEP REASONING

## Generated Data/Instruction/Knowledge
- Generated Knowledge Prompting for Commonsense Reasoning
- Generating Training Data with Language Models: Towards Zero-Shot Language Understanding
- SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions
- GENERATE RATHER THAN RETRIEVE: LARGE LANGUAGE MODELS ARE STRONG CONTEXT GENERATORS



